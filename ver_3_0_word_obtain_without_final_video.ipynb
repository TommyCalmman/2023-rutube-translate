{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYAaAJtxfYIn"
      },
      "outputs": [],
      "source": [
        "!pip install TTS\n",
        "!pip install -U openai-whisper\n",
        "!pip install moviepy\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.api import TTS\n",
        "# from TTS.api import TTS\n",
        "import whisper\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips, concatenate_audioclips\n",
        "# # from deep_translator import GoogleTranslator\n",
        "# import os\n",
        "# # from gtts import gTTS\n",
        "# from transformers import pipeline\n",
        "# # import spacy\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model_type_stt=\"medium\"\n",
        "# # Load the Whisper model\n",
        "model = whisper.load_model(model_type_stt)\n",
        "\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=False)\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "model_t = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "79qDX4x2ffxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_langs_t = {'Английский': 'eng_Latn',\n",
        "            'Датский': 'dan_Latn',\n",
        "            'Испанский': 'spa_Latn',\n",
        "            'Итальянский': 'ita_Latn',\n",
        "            'Китайский': 'zho_Hans',\n",
        "            'Немецкий': 'deu_Latn',\n",
        "            'Польский': 'pol_Latn',\n",
        "            'Португальский': 'por_Latn',\n",
        "            'Турецкий': 'tur_Latn',\n",
        "            'Французский': 'fra_Latn',\n",
        "            'Чешский': 'ces_Latn',\n",
        "            # 'Японский': 'jpn_Jpan',\n",
        "            }\n",
        "\n",
        "d_langs_xtts = {'Английский': 'en',\n",
        "            'Датский': 'nl',\n",
        "            'Испанский': 'es',\n",
        "            'Итальянский': 'it',\n",
        "            'Китайский': 'zh-cn',\n",
        "            'Немецкий': 'de',\n",
        "            'Польский': 'pl',\n",
        "            'Португальский': 'pt',\n",
        "            'Турецкий': 'tr',\n",
        "            'Французский': 'fr',\n",
        "            'Чешский': 'cs',\n",
        "            # 'Японский': 'ja',\n",
        "            }\n",
        "\n",
        "def extract_audio_from_video(video_file, output_audio_format=\"mp3\"):\n",
        "    video = VideoFileClip(video_file)\n",
        "    audio_file = f\"{video_file.rsplit('.', 1)[0]}.{output_audio_format}\"\n",
        "    video.audio.write_audiofile(audio_file, logger=None)  # Disable logging for faster I/O\n",
        "    return audio_file\n",
        "\n",
        "def transcribe_with_whisper(audio_file, model=model):\n",
        "    # Transcribe the audio\n",
        "    result = model.transcribe(audio_file)\n",
        "    return result\n",
        "\n",
        "def speech2text(video_file):\n",
        "    audio_file = extract_audio_from_video(video_file)\n",
        "    data = transcribe_with_whisper(audio_file)\n",
        "    return {\n",
        "        'timecode_with_text': [[x['start'], x['end'], x['text']] for x in data['segments']],\n",
        "        'text': data['text']\n",
        "    }\n",
        "\n",
        "def cut_video(video_path, intervals):\n",
        "    video = VideoFileClip(video_path)\n",
        "    clips = []\n",
        "\n",
        "    start, end = intervals[0], intervals[1]\n",
        "    clip = video.subclip(start, end)\n",
        "    clips.append(clip)\n",
        "\n",
        "    return clips\n",
        "\n",
        "def concat_video_and_wav(clip):\n",
        "        # Load the video\n",
        "    # video = VideoFileClip(video_path)\n",
        "\n",
        "    # Load the audio file\n",
        "    audio = AudioFileClip('./wavs_for_xtts/wav_translated.wav')\n",
        "\n",
        "    # Set the audio of the video\n",
        "    return clip[0].set_audio(audio)\n",
        "# video_gen_path = '/content/Wav2Lip/results/result_voice.mp4'\n",
        "\n",
        "def get_wav_translate(text, path_to_speaker, lang):\n",
        "    # generate speech by cloning a voice using default settings\n",
        "    print(text, path_to_speaker, lang)\n",
        "    tts.tts_to_file(text=text,\n",
        "                    file_path=\"./wavs_for_xtts/wav_translated.wav\",\n",
        "                    speaker_wav=path_to_speaker,\n",
        "                    language=lang)\n",
        "\n",
        "def concatenate_to_wav(clips, path='./concat_wavs'):\n",
        "    audio_clips = []\n",
        "    for clip in clips:\n",
        "        audio_clips.append(clip.audio)\n",
        "    final_clip = concatenate_audioclips(audio_clips)\n",
        "    final_clip.write_audiofile(path + '/wav_after_concat.wav')\n",
        "\n",
        "def translate(text, target_language):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model_t.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_language])\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "sHvA4scBf44A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(video_file, lang='Английский'):\n",
        "    lang_t = d_langs_t.get(lang, 'eng_Latn')\n",
        "    lang_xtts = d_langs_xtts.get(lang, 'en')\n",
        "    d_text_timecodes = speech2text(video_file)\n",
        "    Path(f\"./wavs_for_xtts\").mkdir(exist_ok = True)\n",
        "    Path(f\"./concat_wavs\").mkdir(exist_ok = True)\n",
        "    Path(f\"./result_videos\").mkdir(exist_ok = True)\n",
        "\n",
        "    name = Path(video_file).stem\n",
        "    lst_clips = []\n",
        "\n",
        "    for intervals in d_text_timecodes['timecode_with_text']:\n",
        "        clips = cut_video(video_file, intervals)\n",
        "        concatenate_to_wav(clips)\n",
        "        get_wav_translate(translate(intervals[-1], lang_t)[0], './concat_wavs/wav_after_concat.wav', lang_xtts)\n",
        "        lst_clips.append(concat_video_and_wav(clips))\n",
        "\n",
        "    # Concatenate the video clips\n",
        "    final_clip = concatenate_videoclips(lst_clips)\n",
        "\n",
        "    # Write the final clip to an mp4 file\n",
        "    final_clip.write_videofile(f\"./result_videos/{name}.mp4\")\n",
        "\n",
        "    return 'OK'\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    video_file = \"/content/drive/MyDrive/wav2lip/33.mp4\"  # Update with your video file path\n",
        "    lang = 'Японский'\n",
        "    print(main(video_file, lang))"
      ],
      "metadata": {
        "id": "x9qnZ0KzgBDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}